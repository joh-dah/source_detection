{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import ndlib.models.epidemics as ep\n",
    "import ndlib.models.ModelConfig as mc\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import utils\n",
    "import numpy as np\n",
    "from torch_geometric.utils.convert import from_networkx\n",
    "import torch\n",
    "from torch.nn import Linear\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GCNConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directed graph: False\n",
      "Number of nodes: 100\n",
      "Number of edges: 502\n"
     ]
    }
   ],
   "source": [
    "g = nx.erdos_renyi_graph(100, 0.1, seed = 123)\n",
    "model = ep.SIRModel(g)\n",
    "\n",
    "# Model Configuration\n",
    "config = mc.Configuration()\n",
    "config.add_model_parameter('beta', 0.006)\n",
    "config.add_model_parameter('gamma', 0.01)\n",
    "config.add_model_parameter(\"fraction_infected\", 0.1)\n",
    "model.set_initial_status(config)\n",
    "\n",
    "iterations = model.iteration_bunch(30)\n",
    "trends = model.build_trends(iterations)\n",
    "\n",
    "utils.print_graph_info(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 3\n",
    "num_features = 3\n",
    "\n",
    "x = nn.functional.one_hot(torch.tensor(list(model.status.values())), num_features).float()\n",
    "y = nn.functional.one_hot(torch.tensor(list(model.initial_status.values())), num_features).float()\n",
    "\n",
    "data = from_networkx(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "  def __init__(self):\n",
    "    super(GCN, self).__init__()\n",
    "    torch.manual_seed(42)\n",
    "    self.conv1 = GCNConv(num_features, 3)\n",
    "    self.conv2 = GCNConv(3, 6)\n",
    "    self.conv3 = GCNConv(6, 3)\n",
    "    self.conv4 = GCNConv(3, 3)\n",
    "    self.conv5 = GCNConv(3, 3)\n",
    "    self.conv6 = GCNConv(3, 3)\n",
    "    self.conv7 = GCNConv(3, 3)\n",
    "    self.classifier = Linear(3, num_classes)\n",
    "  def forward(self, x, edge_index):\n",
    "    h = self.conv1(x, edge_index)\n",
    "    h = h.tanh()\n",
    "    h = self.conv2(h, edge_index)\n",
    "    h = h.tanh()\n",
    "    h = self.conv3(h, edge_index)\n",
    "    h = h.tanh()\n",
    "    h = self.conv4(h, edge_index)\n",
    "    h = h.tanh()\n",
    "    h = self.conv5(h, edge_index)\n",
    "    h = h.tanh()\n",
    "    h = self.conv6(h, edge_index)\n",
    "    h = h.tanh()\n",
    "    h = self.conv7(h, edge_index)\n",
    "    out = self.classifier(h)\n",
    "    return out, h\n",
    "  \n",
    "model = GCN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\tLoss: 1.2846\n",
      "Epoch: 2\tLoss: 1.1764\n",
      "Epoch: 3\tLoss: 0.9923\n",
      "Epoch: 4\tLoss: 0.8212\n",
      "Epoch: 5\tLoss: 0.6944\n",
      "Epoch: 6\tLoss: 0.5792\n",
      "Epoch: 7\tLoss: 0.4702\n",
      "Epoch: 8\tLoss: 0.3905\n",
      "Epoch: 9\tLoss: 0.3619\n",
      "Epoch: 10\tLoss: 0.3758\n",
      "Epoch: 11\tLoss: 0.4000\n",
      "Epoch: 12\tLoss: 0.4107\n",
      "Epoch: 13\tLoss: 0.4013\n",
      "Epoch: 14\tLoss: 0.3755\n",
      "Epoch: 15\tLoss: 0.3438\n",
      "Epoch: 16\tLoss: 0.3268\n",
      "Epoch: 17\tLoss: 0.3474\n",
      "Epoch: 18\tLoss: 0.3732\n",
      "Epoch: 19\tLoss: 0.3551\n",
      "Epoch: 20\tLoss: 0.3303\n",
      "Epoch: 21\tLoss: 0.3267\n",
      "Epoch: 22\tLoss: 0.3367\n",
      "Epoch: 23\tLoss: 0.3466\n",
      "Epoch: 24\tLoss: 0.3495\n",
      "Epoch: 25\tLoss: 0.3447\n",
      "Epoch: 26\tLoss: 0.3356\n",
      "Epoch: 27\tLoss: 0.3275\n",
      "Epoch: 28\tLoss: 0.3255\n",
      "Epoch: 29\tLoss: 0.3304\n",
      "Epoch: 30\tLoss: 0.3359\n",
      "Epoch: 31\tLoss: 0.3354\n",
      "Epoch: 32\tLoss: 0.3300\n",
      "Epoch: 33\tLoss: 0.3257\n",
      "Epoch: 34\tLoss: 0.3254\n",
      "Epoch: 35\tLoss: 0.3277\n",
      "Epoch: 36\tLoss: 0.3300\n",
      "Epoch: 37\tLoss: 0.3307\n",
      "Epoch: 38\tLoss: 0.3295\n",
      "Epoch: 39\tLoss: 0.3273\n",
      "Epoch: 40\tLoss: 0.3255\n",
      "Epoch: 41\tLoss: 0.3251\n",
      "Epoch: 42\tLoss: 0.3261\n",
      "Epoch: 43\tLoss: 0.3274\n",
      "Epoch: 44\tLoss: 0.3277\n",
      "Epoch: 45\tLoss: 0.3269\n",
      "Epoch: 46\tLoss: 0.3258\n",
      "Epoch: 47\tLoss: 0.3251\n",
      "Epoch: 48\tLoss: 0.3252\n",
      "Epoch: 49\tLoss: 0.3258\n",
      "Epoch: 50\tLoss: 0.3262\n",
      "Epoch: 51\tLoss: 0.3263\n",
      "Epoch: 52\tLoss: 0.3259\n",
      "Epoch: 53\tLoss: 0.3253\n",
      "Epoch: 54\tLoss: 0.3250\n",
      "Epoch: 55\tLoss: 0.3250\n",
      "Epoch: 56\tLoss: 0.3253\n",
      "Epoch: 57\tLoss: 0.3255\n",
      "Epoch: 58\tLoss: 0.3254\n",
      "Epoch: 59\tLoss: 0.3251\n",
      "Epoch: 60\tLoss: 0.3248\n",
      "Epoch: 61\tLoss: 0.3247\n",
      "Epoch: 62\tLoss: 0.3248\n",
      "Epoch: 63\tLoss: 0.3250\n",
      "Epoch: 64\tLoss: 0.3250\n",
      "Epoch: 65\tLoss: 0.3249\n",
      "Epoch: 66\tLoss: 0.3247\n",
      "Epoch: 67\tLoss: 0.3245\n",
      "Epoch: 68\tLoss: 0.3246\n",
      "Epoch: 69\tLoss: 0.3246\n",
      "Epoch: 70\tLoss: 0.3246\n",
      "Epoch: 71\tLoss: 0.3245\n",
      "Epoch: 72\tLoss: 0.3244\n",
      "Epoch: 73\tLoss: 0.3243\n",
      "Epoch: 74\tLoss: 0.3243\n",
      "Epoch: 75\tLoss: 0.3243\n",
      "Epoch: 76\tLoss: 0.3243\n",
      "Epoch: 77\tLoss: 0.3242\n",
      "Epoch: 78\tLoss: 0.3241\n",
      "Epoch: 79\tLoss: 0.3241\n",
      "Epoch: 80\tLoss: 0.3241\n",
      "Epoch: 81\tLoss: 0.3240\n",
      "Epoch: 82\tLoss: 0.3239\n",
      "Epoch: 83\tLoss: 0.3238\n",
      "Epoch: 84\tLoss: 0.3238\n",
      "Epoch: 85\tLoss: 0.3238\n",
      "Epoch: 86\tLoss: 0.3237\n",
      "Epoch: 87\tLoss: 0.3236\n",
      "Epoch: 88\tLoss: 0.3236\n",
      "Epoch: 89\tLoss: 0.3235\n",
      "Epoch: 90\tLoss: 0.3234\n",
      "Epoch: 91\tLoss: 0.3233\n",
      "Epoch: 92\tLoss: 0.3233\n",
      "Epoch: 93\tLoss: 0.3232\n",
      "Epoch: 94\tLoss: 0.3231\n",
      "Epoch: 95\tLoss: 0.3230\n",
      "Epoch: 96\tLoss: 0.3229\n",
      "Epoch: 97\tLoss: 0.3228\n",
      "Epoch: 98\tLoss: 0.3228\n",
      "Epoch: 99\tLoss: 0.3226\n",
      "Epoch: 100\tLoss: 0.3225\n",
      "Epoch: 101\tLoss: 0.3224\n",
      "Epoch: 102\tLoss: 0.3223\n",
      "Epoch: 103\tLoss: 0.3222\n",
      "Epoch: 104\tLoss: 0.3221\n",
      "Epoch: 105\tLoss: 0.3219\n",
      "Epoch: 106\tLoss: 0.3218\n",
      "Epoch: 107\tLoss: 0.3217\n",
      "Epoch: 108\tLoss: 0.3215\n",
      "Epoch: 109\tLoss: 0.3214\n",
      "Epoch: 110\tLoss: 0.3212\n",
      "Epoch: 111\tLoss: 0.3210\n",
      "Epoch: 112\tLoss: 0.3209\n",
      "Epoch: 113\tLoss: 0.3207\n",
      "Epoch: 114\tLoss: 0.3205\n",
      "Epoch: 115\tLoss: 0.3203\n",
      "Epoch: 116\tLoss: 0.3201\n",
      "Epoch: 117\tLoss: 0.3199\n",
      "Epoch: 118\tLoss: 0.3196\n",
      "Epoch: 119\tLoss: 0.3194\n",
      "Epoch: 120\tLoss: 0.3191\n",
      "Epoch: 121\tLoss: 0.3189\n",
      "Epoch: 122\tLoss: 0.3186\n",
      "Epoch: 123\tLoss: 0.3183\n",
      "Epoch: 124\tLoss: 0.3180\n",
      "Epoch: 125\tLoss: 0.3177\n",
      "Epoch: 126\tLoss: 0.3174\n",
      "Epoch: 127\tLoss: 0.3171\n",
      "Epoch: 128\tLoss: 0.3171\n",
      "Epoch: 129\tLoss: 0.3197\n",
      "Epoch: 130\tLoss: 0.3294\n",
      "Epoch: 131\tLoss: 0.3174\n",
      "Epoch: 132\tLoss: 0.3211\n",
      "Epoch: 133\tLoss: 0.3201\n",
      "Epoch: 134\tLoss: 0.3192\n",
      "Epoch: 135\tLoss: 0.3168\n",
      "Epoch: 136\tLoss: 0.3189\n",
      "Epoch: 137\tLoss: 0.3153\n",
      "Epoch: 138\tLoss: 0.3182\n",
      "Epoch: 139\tLoss: 0.3159\n",
      "Epoch: 140\tLoss: 0.3154\n",
      "Epoch: 141\tLoss: 0.3167\n",
      "Epoch: 142\tLoss: 0.3143\n",
      "Epoch: 143\tLoss: 0.3158\n",
      "Epoch: 144\tLoss: 0.3150\n",
      "Epoch: 145\tLoss: 0.3139\n",
      "Epoch: 146\tLoss: 0.3151\n",
      "Epoch: 147\tLoss: 0.3132\n",
      "Epoch: 148\tLoss: 0.3143\n",
      "Epoch: 149\tLoss: 0.3130\n",
      "Epoch: 150\tLoss: 0.3133\n",
      "Epoch: 151\tLoss: 0.3126\n",
      "Epoch: 152\tLoss: 0.3125\n",
      "Epoch: 153\tLoss: 0.3122\n",
      "Epoch: 154\tLoss: 0.3117\n",
      "Epoch: 155\tLoss: 0.3115\n",
      "Epoch: 156\tLoss: 0.3111\n",
      "Epoch: 157\tLoss: 0.3108\n",
      "Epoch: 158\tLoss: 0.3105\n",
      "Epoch: 159\tLoss: 0.3099\n",
      "Epoch: 160\tLoss: 0.3099\n",
      "Epoch: 161\tLoss: 0.3092\n",
      "Epoch: 162\tLoss: 0.3093\n",
      "Epoch: 163\tLoss: 0.3085\n",
      "Epoch: 164\tLoss: 0.3085\n",
      "Epoch: 165\tLoss: 0.3081\n",
      "Epoch: 166\tLoss: 0.3076\n",
      "Epoch: 167\tLoss: 0.3076\n",
      "Epoch: 168\tLoss: 0.3070\n",
      "Epoch: 169\tLoss: 0.3067\n",
      "Epoch: 170\tLoss: 0.3066\n",
      "Epoch: 171\tLoss: 0.3063\n",
      "Epoch: 172\tLoss: 0.3059\n",
      "Epoch: 173\tLoss: 0.3055\n",
      "Epoch: 174\tLoss: 0.3052\n",
      "Epoch: 175\tLoss: 0.3049\n",
      "Epoch: 176\tLoss: 0.3048\n",
      "Epoch: 177\tLoss: 0.3054\n",
      "Epoch: 178\tLoss: 0.3132\n",
      "Epoch: 179\tLoss: 0.3609\n",
      "Epoch: 180\tLoss: 0.3042\n",
      "Epoch: 181\tLoss: 0.3488\n",
      "Epoch: 182\tLoss: 0.3396\n",
      "Epoch: 183\tLoss: 0.3546\n",
      "Epoch: 184\tLoss: 0.3054\n",
      "Epoch: 185\tLoss: 0.3643\n",
      "Epoch: 186\tLoss: 0.3205\n",
      "Epoch: 187\tLoss: 0.3589\n",
      "Epoch: 188\tLoss: 0.3375\n",
      "Epoch: 189\tLoss: 0.3063\n",
      "Epoch: 190\tLoss: 0.3563\n",
      "Epoch: 191\tLoss: 0.3093\n",
      "Epoch: 192\tLoss: 0.3186\n",
      "Epoch: 193\tLoss: 0.3304\n",
      "Epoch: 194\tLoss: 0.3206\n",
      "Epoch: 195\tLoss: 0.3088\n",
      "Epoch: 196\tLoss: 0.3195\n",
      "Epoch: 197\tLoss: 0.3213\n",
      "Epoch: 198\tLoss: 0.3100\n",
      "Epoch: 199\tLoss: 0.3143\n",
      "Epoch: 200\tLoss: 0.3199\n",
      "Epoch: 201\tLoss: 0.3171\n",
      "Epoch: 202\tLoss: 0.3112\n",
      "Epoch: 203\tLoss: 0.3122\n",
      "Epoch: 204\tLoss: 0.3169\n",
      "Epoch: 205\tLoss: 0.3141\n",
      "Epoch: 206\tLoss: 0.3107\n",
      "Epoch: 207\tLoss: 0.3122\n",
      "Epoch: 208\tLoss: 0.3142\n",
      "Epoch: 209\tLoss: 0.3132\n",
      "Epoch: 210\tLoss: 0.3109\n",
      "Epoch: 211\tLoss: 0.3107\n",
      "Epoch: 212\tLoss: 0.3123\n",
      "Epoch: 213\tLoss: 0.3121\n",
      "Epoch: 214\tLoss: 0.3102\n",
      "Epoch: 215\tLoss: 0.3099\n",
      "Epoch: 216\tLoss: 0.3108\n",
      "Epoch: 217\tLoss: 0.3108\n",
      "Epoch: 218\tLoss: 0.3096\n",
      "Epoch: 219\tLoss: 0.3089\n",
      "Epoch: 220\tLoss: 0.3095\n",
      "Epoch: 221\tLoss: 0.3094\n",
      "Epoch: 222\tLoss: 0.3083\n",
      "Epoch: 223\tLoss: 0.3080\n",
      "Epoch: 224\tLoss: 0.3084\n",
      "Epoch: 225\tLoss: 0.3080\n",
      "Epoch: 226\tLoss: 0.3072\n",
      "Epoch: 227\tLoss: 0.3071\n",
      "Epoch: 228\tLoss: 0.3073\n",
      "Epoch: 229\tLoss: 0.3066\n",
      "Epoch: 230\tLoss: 0.3063\n",
      "Epoch: 231\tLoss: 0.3064\n",
      "Epoch: 232\tLoss: 0.3060\n",
      "Epoch: 233\tLoss: 0.3055\n",
      "Epoch: 234\tLoss: 0.3056\n",
      "Epoch: 235\tLoss: 0.3053\n",
      "Epoch: 236\tLoss: 0.3049\n",
      "Epoch: 237\tLoss: 0.3050\n",
      "Epoch: 238\tLoss: 0.3047\n",
      "Epoch: 239\tLoss: 0.3044\n",
      "Epoch: 240\tLoss: 0.3044\n",
      "Epoch: 241\tLoss: 0.3041\n",
      "Epoch: 242\tLoss: 0.3040\n",
      "Epoch: 243\tLoss: 0.3039\n",
      "Epoch: 244\tLoss: 0.3036\n",
      "Epoch: 245\tLoss: 0.3037\n",
      "Epoch: 246\tLoss: 0.3034\n",
      "Epoch: 247\tLoss: 0.3034\n",
      "Epoch: 248\tLoss: 0.3033\n",
      "Epoch: 249\tLoss: 0.3031\n",
      "Epoch: 250\tLoss: 0.3031\n",
      "Epoch: 251\tLoss: 0.3030\n",
      "Epoch: 252\tLoss: 0.3030\n",
      "Epoch: 253\tLoss: 0.3028\n",
      "Epoch: 254\tLoss: 0.3028\n",
      "Epoch: 255\tLoss: 0.3027\n",
      "Epoch: 256\tLoss: 0.3027\n",
      "Epoch: 257\tLoss: 0.3026\n",
      "Epoch: 258\tLoss: 0.3026\n",
      "Epoch: 259\tLoss: 0.3026\n",
      "Epoch: 260\tLoss: 0.3025\n",
      "Epoch: 261\tLoss: 0.3025\n",
      "Epoch: 262\tLoss: 0.3024\n",
      "Epoch: 263\tLoss: 0.3024\n",
      "Epoch: 264\tLoss: 0.3024\n",
      "Epoch: 265\tLoss: 0.3023\n",
      "Epoch: 266\tLoss: 0.3023\n",
      "Epoch: 267\tLoss: 0.3023\n",
      "Epoch: 268\tLoss: 0.3022\n",
      "Epoch: 269\tLoss: 0.3021\n",
      "Epoch: 270\tLoss: 0.3021\n",
      "Epoch: 271\tLoss: 0.3021\n",
      "Epoch: 272\tLoss: 0.3021\n",
      "Epoch: 273\tLoss: 0.3021\n",
      "Epoch: 274\tLoss: 0.3022\n",
      "Epoch: 275\tLoss: 0.3025\n",
      "Epoch: 276\tLoss: 0.3034\n",
      "Epoch: 277\tLoss: 0.3078\n",
      "Epoch: 278\tLoss: 0.3164\n",
      "Epoch: 279\tLoss: 0.3404\n",
      "Epoch: 280\tLoss: 0.3020\n",
      "Epoch: 281\tLoss: 0.3315\n",
      "Epoch: 282\tLoss: 0.3158\n",
      "Epoch: 283\tLoss: 0.3272\n",
      "Epoch: 284\tLoss: 0.3030\n",
      "Epoch: 285\tLoss: 0.3268\n",
      "Epoch: 286\tLoss: 0.3034\n",
      "Epoch: 287\tLoss: 0.3194\n",
      "Epoch: 288\tLoss: 0.3116\n",
      "Epoch: 289\tLoss: 0.3076\n",
      "Epoch: 290\tLoss: 0.3147\n",
      "Epoch: 291\tLoss: 0.3052\n",
      "Epoch: 292\tLoss: 0.3139\n",
      "Epoch: 293\tLoss: 0.3065\n",
      "Epoch: 294\tLoss: 0.3070\n",
      "Epoch: 295\tLoss: 0.3103\n",
      "Epoch: 296\tLoss: 0.3041\n",
      "Epoch: 297\tLoss: 0.3081\n",
      "Epoch: 298\tLoss: 0.3077\n",
      "Epoch: 299\tLoss: 0.3042\n"
     ]
    }
   ],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "def train(data, x, y):\n",
    "  optimizer.zero_grad()\n",
    "  out, h = model(x, data.edge_index)\n",
    "  loss = criterion(out, y)\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "  return loss, h\n",
    "epochs = range(1, 300)\n",
    "losses = []\n",
    "embeddings = []\n",
    "for epoch in epochs:\n",
    "  loss, h = train(data, x, y)\n",
    "  losses.append(loss)\n",
    "  embeddings.append(h)\n",
    "  print(f\"Epoch: {epoch}\\tLoss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
